{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping INDEED to get job listings.\n",
    "\n",
    "In this notebook I am scraping the Indeed website for Data Scientist, Data Analyst and Data Engineer jobs in key cities from US, UK, SG, HK and AUS. I have split the scraper up into 3 functions (extractor, params, and scraper) which returns a pandas dataframe object for each country being queried.\n",
    "\n",
    "---\n",
    "\n",
    "The main function is the __scraper__ function. It goes to the urls defined from the __params__ functions and uses the __extractor__ function to retrieve information in the url to store into a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the extractor function. \n",
    "This function will be called from the soup that is created from querying the html and returns a dictionary with the scraped information and the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractor(listings, scraped_data, city):\n",
    "    \"\"\" extractor function. Takes a page of listings and makes lists of:\n",
    "        title, company, location, description, experience, salary, and city.\n",
    "        listings come from the soup object, scraped data is the dictionary defined in the scraper function, and\n",
    "        city is a string of the city that comes from the params function.\"\"\"\n",
    "    \n",
    "    \n",
    "    def get_title(listing):\n",
    "        \"\"\" gets job title\"\"\"\n",
    "        try:\n",
    "            return listing.find('a', {'data-tn-element':'jobTitle'}).get('title').strip()\n",
    "        except:\n",
    "            print(str())\n",
    "            return 'NA'\n",
    "        \n",
    "    def get_company(listing):\n",
    "        \"\"\" gets company\"\"\"\n",
    "        try:\n",
    "            return listing.find('span', {'class':'company'}).text.strip()\n",
    "        except:\n",
    "            return 'NA'\n",
    "        \n",
    "    def get_location(listing):\n",
    "        \"\"\" gets location\"\"\"\n",
    "        try:\n",
    "            return listing.find('span', {'class': 'location'}).text.strip()\n",
    "        except:\n",
    "            return 'NA'\n",
    "        \n",
    "    def get_description(listing):\n",
    "        \"\"\" description \"\"\"\n",
    "        try:\n",
    "            return listing.find('span', {'class': 'summary'}).text.strip()\n",
    "        except:\n",
    "            return 'NA'\n",
    "    \n",
    "    def get_desired_exp(listing):\n",
    "        \"\"\" desired exp \"\"\"\n",
    "        try:\n",
    "            return listing.find('span', {'class':'experienceList'}).text.strip()\n",
    "        except:\n",
    "            return 'NA'\n",
    "        \n",
    "    def get_salary(listing):\n",
    "        \"\"\" salary if available \"\"\"\n",
    "        try:\n",
    "            return listing.find('span', {'class': 'no-wrap'}).text.strip()\n",
    "        except:\n",
    "            return 'NA'\n",
    "        \n",
    "    def get_reviewcount(listing):\n",
    "        \"\"\" number of reviews of company\"\"\"\n",
    "        try:\n",
    "            result = listing.find('a', {'data-tn-element':'reviewStars'})\n",
    "            return result.find('span', {'class','slNoUnderline'}).text.strip()\n",
    "        except:\n",
    "            return 'NA'\n",
    "    \n",
    "    def get_stars(listing):\n",
    "        \"\"\" stars of company in terms of pixelwidth\"\"\"\n",
    "        try:\n",
    "            result = listing.find('span', {'class':'rating'}).get('style').strip()\n",
    "            return result.replace('width:', '').replace('px', '')\n",
    "        except:\n",
    "            return 'NA'\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    for listing in listings:\n",
    "        scraped_data['title'].append(get_title(listing))\n",
    "        scraped_data['company'].append(get_company(listing))\n",
    "        scraped_data['location'].append(get_location(listing))\n",
    "        scraped_data['description'].append(get_description(listing))\n",
    "        scraped_data['experience'].append(get_desired_exp(listing))\n",
    "        scraped_data['salary'].append(get_salary(listing))\n",
    "        scraped_data['reviews'].append(get_reviewcount(listing))\n",
    "        scraped_data['stars'].append(get_stars(listing))\n",
    "        scraped_data['city'].append(city)\n",
    "    \n",
    "    return scraped_data\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The params function. \n",
    "The params function gets the URL of the site for me to scrape, along with the cities that are defined in the scraper function below. It returns a tuple with the URL to be scraped, along with the city associated with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def params(country, dictofcities):\n",
    "    \"\"\" returns the url and city in a tuple based on name of city. \n",
    "        url contains Data Scientist, Data Analyst and Data Engineer with the keywords Machine Learning\n",
    "        , Artificial Intellignece and Analytics\"\"\"\n",
    "\n",
    "    assert country in ['SG','UK','US','HK','AU','CA'] # throws exception error if invalid country\n",
    "    \n",
    "    if country in 'SG':\n",
    "        base_url = \"https://www.indeed.com.sg/jobs\"\n",
    "    elif country in 'UK':\n",
    "        base_url = \"https://www.indeed.co.uk/jobs\"\n",
    "    elif country in 'US':\n",
    "        base_url = \"https://www.indeed.com/jobs\"\n",
    "    elif country in 'HK':\n",
    "        base_url = \"https://www.indeed.hk/jobs\"\n",
    "    elif country in 'AU':\n",
    "        base_url = \"https://au.indeed.com/jobs\"\n",
    "    elif country in 'CA':\n",
    "        base_url = \"https://ca.indeed.com/jobs\"      \n",
    "    \n",
    "    \n",
    "    cities = dictofcities[country]   # Retrieves the city list. Look at scraper function to see what the dict is.\n",
    "    urls = []\n",
    "    citylist = []      #appends the city name\n",
    "    for city in cities:\n",
    "        urls.append(base_url + \"?q=Data+Scientist+(machine+or+Learning+or+Analytics+or+Artificial+or+Intelligence)\" \\\n",
    "        + \"&l={}&limit=50\".format('+'.join(city.split())))\n",
    "        citylist.append(city)\n",
    "        urls.append(base_url + \"?q=Data+Analyst+(machine+or+Learning+or+Analytics+or+Artificial+or+Intelligence)\" \\\n",
    "        + \"&l={}&limit=50\".format('+'.join(city.split())))\n",
    "        citylist.append(city)\n",
    "        urls.append(base_url + \"?q=Data+Engineer+(machine+or+Learning+or+Analytics+or+Artificial+or+Intelligence)\" \\\n",
    "        + \"&l={}&limit=50\".format('+'.join(city.split())))\n",
    "        citylist.append(city)\n",
    "        \n",
    "        # '+'.join(city.split()) had to be used to split the city name up into a chunk to be inserted into the URL.\n",
    "        # Eg: \"New York\" --> \"New+York\"\n",
    "        \n",
    "    return zip(urls, citylist)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me test the params function here. seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London\n",
      "London\n",
      "London\n",
      "Manchester\n",
      "Manchester\n",
      "Manchester\n",
      "Birmingham\n",
      "Birmingham\n",
      "Birmingham\n",
      "Liverpool\n",
      "Liverpool\n",
      "Liverpool\n",
      "Milton Keynes\n",
      "Milton Keynes\n",
      "Milton Keynes\n",
      "Bristol\n",
      "Bristol\n",
      "Bristol\n",
      "Cambridge\n",
      "Cambridge\n",
      "Cambridge\n",
      "Oxford\n",
      "Oxford\n",
      "Oxford\n"
     ]
    }
   ],
   "source": [
    "dictofcities = {'SG':['Singapore'],\n",
    "                   'UK':['London','Manchester','Birmingham','Liverpool','Milton Keynes','Bristol','Cambridge','Oxford'],\n",
    "                   'US':['Houston','San Francisco','Mountain View','Palo Alto','Los Angeles','New York', 'San Jose',\n",
    "                        'Boston','Chicago','Seattle','Austin','Dallas','San Diego','Denver','Portland','St Louis',\n",
    "                        'Philadelphia','Cincinnati'],\n",
    "                   'CA':['Toronto', 'Montreal','Vancouver'],\n",
    "                   'AU':['Sydney','Melbourne','Brisbane','Perth'],\n",
    "                   'HK':['Hong Kong']}\n",
    "\n",
    "\n",
    "test = params('UK',dictofcities)\n",
    "for t in test:\n",
    "    print t[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the scraper function.\n",
    "\n",
    "This function basically calls the params and extractor function to get a dataframe of job listings for the country being called in the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scraper(country):\n",
    "    \"\"\" scraper function. parameters have to be defined beforehand. Returns a dataframe that contains job\n",
    "    listings. \"\"\"\n",
    "    \n",
    "    \n",
    "    print \"Current system time: {}\".format(time.ctime())   # prints current time\n",
    "    start_time = time.time()   # starts timer\n",
    "    \n",
    "    scraped_data = {'title': [],           # This dict will eventually be converted into a pandas dataframe.\n",
    "                   'company': [],       \n",
    "                   'location': [],\n",
    "                   'description': [],\n",
    "                   'experience': [],\n",
    "                   'salary': [],\n",
    "                   'reviews': [],\n",
    "                   'stars': [],\n",
    "                   'city': []}\n",
    "    \n",
    "    dictofcities = {'SG':['Singapore'],     # dict of cities to search\n",
    "                   'UK':['London','Manchester','Birmingham','Liverpool','Milton Keynes','Bristol','Cambridge','Oxford',\n",
    "                        'Birmingham','Leeds','Brighton','Southampton'],\n",
    "                   'US':['Houston','San Francisco','Mountain View','Palo Alto','Los Angeles','New York', 'San Jose',\n",
    "                        'Boston','Chicago','Seattle','Austin','Dallas','San Diego','Denver','Portland','St Louis',\n",
    "                        'Philadelphia','Cincinnati','Atlanta','Berkeley','Detroit','Miami',''],\n",
    "                   'CA':['Toronto', 'Montreal','Vancouver','Ottawa'],\n",
    "                   'AU':['Sydney','Melbourne','Brisbane','Perth'],\n",
    "                   'HK':['Hong Kong']}\n",
    "    \n",
    "\n",
    "    links = params(country, dictofcities)      # gets the URLs for each site to scrape\n",
    "    print \"Scraping Indeed for country: {}\".format(country)\n",
    "    for link in links:\n",
    "        for i in range(0,1000,50):            # iterates through all the pages up to a max of 1000 results\n",
    "            url = link[0] + \"&start={}\".format(i)\n",
    "    \n",
    "            html = requests.get(url)\n",
    "            assert html.status_code == requests.codes.ok\n",
    "    \n",
    "            soup = BeautifulSoup(html.text, \"lxml\")\n",
    "            listings = soup.find_all('div', {'data-tn-component':'organicJob'}) #organicJob would not take the sponsored content\n",
    "        \n",
    "            scraped_data = extractor(listings, scraped_data, link[1])  # look at extractor function. \n",
    "            \n",
    "    df = pd.DataFrame(scraped_data)      # makes a dataframe for the country\n",
    "    dfdropped = df.drop_duplicates()     # drops duplicates\n",
    "    run_time = time.time() - start_time  # stops timer\n",
    "    print \"Time elapsed in seconds: {}\".format(run_time)\n",
    "    print \"Scraper finished at: {}\".format(time.ctime())\n",
    "    return dfdropped\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system time: Thu Oct 26 14:57:13 2017\n",
      "Scraping Indeed for country: SG\n",
      "Time elapsed in seconds: 61.4437291622\n",
      "Scraper finished at: Thu Oct 26 14:58:14 2017\n"
     ]
    }
   ],
   "source": [
    "dfsg = scraper('SG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>experience</th>\n",
       "      <th>location</th>\n",
       "      <th>reviews</th>\n",
       "      <th>salary</th>\n",
       "      <th>stars</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>Libbler</td>\n",
       "      <td>Good understanding of investment and quant, ma...</td>\n",
       "      <td>NA</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Research Data Manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>Procter &amp; Gamble</td>\n",
       "      <td>Scope includes acquiring, cleaning, formatting...</td>\n",
       "      <td>NA</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>3,713 reviews</td>\n",
       "      <td>NA</td>\n",
       "      <td>52.2</td>\n",
       "      <td>Research &amp; Development - Data Transformation S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>EkkBaz.com</td>\n",
       "      <td>Selecting features, building and optimizing cl...</td>\n",
       "      <td>NA</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>NA</td>\n",
       "      <td>$1,000 - $2,000 a month</td>\n",
       "      <td>NA</td>\n",
       "      <td>Data Scientist Intern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>IBM</td>\n",
       "      <td>Familiar with at least one Machine Learning li...</td>\n",
       "      <td>NA</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>18,106 reviews</td>\n",
       "      <td>NA</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Research Data Scientist (IBM Singapore Lab)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>GIC Investment</td>\n",
       "      <td>Data &amp; Analytics Department. Experience buildi...</td>\n",
       "      <td>NA</td>\n",
       "      <td>Tanjong Pagar</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>AVP/VP, Machine Learning Engineer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        city           company  \\\n",
       "0  Singapore           Libbler   \n",
       "1  Singapore  Procter & Gamble   \n",
       "2  Singapore        EkkBaz.com   \n",
       "3  Singapore               IBM   \n",
       "4  Singapore    GIC Investment   \n",
       "\n",
       "                                         description experience  \\\n",
       "0  Good understanding of investment and quant, ma...         NA   \n",
       "1  Scope includes acquiring, cleaning, formatting...         NA   \n",
       "2  Selecting features, building and optimizing cl...         NA   \n",
       "3  Familiar with at least one Machine Learning li...         NA   \n",
       "4  Data & Analytics Department. Experience buildi...         NA   \n",
       "\n",
       "        location         reviews                   salary stars  \\\n",
       "0      Singapore              NA                       NA    NA   \n",
       "1      Singapore   3,713 reviews                       NA  52.2   \n",
       "2     Ang Mo Kio              NA  $1,000 - $2,000 a month    NA   \n",
       "3      Singapore  18,106 reviews                       NA  51.0   \n",
       "4  Tanjong Pagar              NA                       NA    NA   \n",
       "\n",
       "                                               title  \n",
       "0                              Research Data Manager  \n",
       "1  Research & Development - Data Transformation S...  \n",
       "2                              Data Scientist Intern  \n",
       "3        Research Data Scientist (IBM Singapore Lab)  \n",
       "4                  AVP/VP, Machine Learning Engineer  "
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system time: Thu Oct 26 14:59:49 2017\n",
      "Scraping Indeed for country: UK\n",
      "Time elapsed in seconds: 2340.02391911\n",
      "Scraper finished at: Thu Oct 26 15:38:49 2017\n"
     ]
    }
   ],
   "source": [
    "dfuk = scraper('UK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6687, 9)"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfuk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsg.to_pickle(\"dfsg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfuk.to_pickle(\"dfuk.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system time: Thu Oct 26 15:42:00 2017\n",
      "Scraping Indeed for country: US\n",
      "Time elapsed in seconds: 3492.91553903\n",
      "Scraper finished at: Thu Oct 26 16:40:13 2017\n"
     ]
    }
   ],
   "source": [
    "dfus = scraper('US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfus.to_pickle(\"dfus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system time: Thu Oct 26 16:46:28 2017\n",
      "Scraping Indeed for country: CA\n",
      "Time elapsed in seconds: 501.877671957\n",
      "Scraper finished at: Thu Oct 26 16:54:50 2017\n"
     ]
    }
   ],
   "source": [
    "dfca = scraper('CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfca.to_pickle(\"dfca.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system time: Thu Oct 26 17:03:53 2017\n",
      "Scraping Indeed for country: AU\n",
      "Time elapsed in seconds: 770.558179855\n",
      "Scraper finished at: Thu Oct 26 17:16:44 2017\n"
     ]
    }
   ],
   "source": [
    "dfau = scraper('AU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfau.to_pickle(\"dfau.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system time: Thu Oct 26 17:58:53 2017\n",
      "Scraping Indeed for country: HK\n",
      "Time elapsed in seconds: 58.5559232235\n",
      "Scraper finished at: Thu Oct 26 17:59:51 2017\n"
     ]
    }
   ],
   "source": [
    "dfhk = scraper('HK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfhk.to_pickle(\"dfhk.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NA                            782\n",
       "$50,000 - $70,000 a month       4\n",
       "$20,000 - $40,000 a month       4\n",
       "$90,000 - $120,000 a month      2\n",
       "$15,000 - $25,000 a month       2\n",
       "$40,000 - $45,000 a month       2\n",
       "$50,000 - $55,000 a month       1\n",
       "$12,000 - $15,000 a month       1\n",
       "$15,000 - $30,000 a month       1\n",
       "Name: salary, dtype: int64"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfhk['salary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
